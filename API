#################################################################################################
#                                  WINDFLOW ACCEPTED SIGNATURES                                 #
#################################################################################################

This file lists all the possible signatures that can be used to create WindFlow operators. In case
you provide a wrong signature during the creation of an operator (through its builder), you receive
a specific error message during the compilation phase (through some static asserts).

For basic and window-based operators, the functional logic, as well as the key extractor and the
closing logic can be provided as functions, lambdas or functor objects providing operator() with
the right signatures.

For GPU operators, the functional logic must be provided as a __host__ __device__ lambda or through
a functor object exposing a __device__ operator() method with the right signature. The key extractor
logic must be provided using a __host__ __device__ lambda or through a functor object with the right
__host__ __device__ operator() method.

SOURCE
------
void(Source_Shipper<tuple_t> &);
void(Source_Shipper<tuple_t> &, RuntimeContext &);

FILTER
------
bool(tuple_t &);
bool(tuple_t &, RuntimeContext &);

FILTER_GPU
----------
[__host__] __device__ bool(tuple_t &);
[__host__] __device__ bool(tuple_t &, state_t &);

MAP
---
void(tuple_t &);
void(tuple_t &, RuntimeContext &);
result_t(const tuple_t &);
result_t(const tuple_t &, RuntimeContext &);

MAP_GPU
-------
[__host__] __device__ void(tuple_t &);
[__host__] __device__ void(tuple_t &, state_t &);

FLATMAP
-------
void(const tuple_t &, Shipper<result_t> &);
void(const tuple_t &, Shipper<result_t> &, RuntimeContext &);

REDUCE
------
void(const tuple_t &, result_t &);
void(const tuple_t &, result_t &, RuntimeContext &);

REDUCE_GPU
----------
[__host__] __device__ tuple_t (const tuple_t &, const tuple_t &);

KEYED_WINDOWS, PARALLEL_WINDOWS
-------------------------------
void(const Iterable<tuple_t> &, result_t &);
void(const Iterable<tuple_t> &, result_t &, RuntimeContext &);
void(const tuple_t &, result_t &);
void(const tuple_t &, result_t &, RuntimeContext &);

PANED_WINDOWS
-------------
The corresponding builder needs two parameters (for the PLQ and WLQ logics) with the following accepted signatures:

 * PLQ
    void(const Iterable<tuple_t> &, tuple_t &);
    void(const Iterable<tuple_t> &, tuple_t &, RuntimeContext &);
    void(const tuple_t &, tuple_t &);
    void(const tuple_t &, tuple_t &, RuntimeContext &);

 * WLQ
    void(const Iterable<tuple_t> &, result_t &);
    void(const Iterable<tuple_t> &, result_t &, RuntimeContext &);
    void(const tuple_t &, result_t &);
    void(const tuple_t &, result_t &, RuntimeContext &);

MAPREDUCE_WINDOWS
-----------------
The corresponding builder needs two parameters (for the MAP and REDUCE logics) with the following accepted signatures:

 * MAP
    void(const Iterable<tuple_t> &, tuple_t &);
    void(const Iterable<tuple_t> &, tuple_t &, RuntimeContext &);
    void(const tuple_t &, tuple_t &);
    void(const tuple_t &, tuple_t &, RuntimeContext &);

 * REDUCE
    void(const Iterable<tuple_t> &, result_t &);
    void(const Iterable<tuple_t> &, result_t &, RuntimeContext &);
    void(const tuple_t &, result_t &);
    void(const tuple_t &, result_t &, RuntimeContext &);

FFAT_AGGREGATOR
---------------
The corresponding builder needs two parameters (for the lift and combine logics) with the following accepted signatures:

 * Lift
    void(const tuple_t &, result_t &);
    void(const tuple_t &, result_t &, RuntimeContext &);

 * Combine
    void(const result_t &, const result_t &, result_t &);
    void(const result_t &, const result_t &, result_t &, RuntimeContext &);

FFAT_AGGREGATOR_GPU
-------------------
The corresponding builder needs two parameters (for the lift and combine logics) with the following accepted signatures:

 * Lift
    void(const tuple_t &, result_t &);

 * Combine
    __host__ __device__ void(const result_t &, const result_t &, result_t &);

SINK
----
void(std::optional<tuple_t> &);
void(std::optional<tuple_t> &, RuntimeContext &);
void(std::optional<std::reference_wrapper<tuple_t>>);
void(std::optional<std::reference_wrapper<tuple_t>>, RuntimeContext &);

CLOSING LOGIC
-------------
void(RuntimeContext &);

KEY EXTRACTOR
-------------
key_t(const tuple_t &); // for all the CPU operators
__host__ __device__ key_t(const tuple_t &); // only for GPU operators

SPLITTING LOGIC
---------------
integral_t(const tuple_t &);
std::vector<integral_t>(const tuple_t &);
integral_t(tuple_t &);
std::vector<integral_t>(tuple_t &);

Where integral_t is any C++ integral type (e.g., short, int, long, and so forth).
